{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605265e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a177e01ba8841aebd9eb9dd8e7fd473f",
     "grade": false,
     "grade_id": "cell-0f8d2f9dca105616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem Set 03\n",
    "\n",
    "## POLI 175 - Machine Learning for Social Scientists\n",
    "\n",
    "### Due Date: 03/10/2023\n",
    "\n",
    "In this problem set we will work with the 2016 wave of the American National Elections Survey (ANES). You can find it in [here](https://electionstudies.org).\n",
    "\n",
    "The file you are going to use for this PS is the `anes2016.csv`. The names of the variables are self-explanatory, and I pre-processed them to cut unimportant variables. I also added informative names.\n",
    "\n",
    "We are going to do two things: first, we are going to predict the vote in the election. Second, we are going to predict two types of swing voters:\n",
    "\n",
    "- Swing voters that answered that would vote for one candidate before the election and then voted for a different one.\n",
    "\n",
    "- Swing voters that voted for one party in the 2012 election but then shifted the choice during the 2016 election.\n",
    "\n",
    "**Content required to solve this homework**: All class content up to Non-Linearity. Specifically:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. KNN\n",
    "3. LDA\n",
    "4. QDA\n",
    "5. Lasso (or L1 regularization)\n",
    "6. Ridge (or L2 regularization)\n",
    "7. GAMs\n",
    "8. Splines\n",
    "9. Cross-Validation: K-Fold and Split-sample CVs.\n",
    "\n",
    "Please reach out if you have any questions.\n",
    "\n",
    "## Loading Packages and pre-processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d97f4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53de3dc91efbf62f32cef8b8d9d80789",
     "grade": false,
     "grade_id": "cell-ad2435cff0a8cfd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Pandas and Numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Seaborn and MatplotLib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# StatsModels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, get_scorer_names\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, SplineTransformer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "dat = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/anes2016.csv')\n",
    "\n",
    "## Predictors\n",
    "cont_predictors = ['pay_attn_pol_cont', 'feel_dem_cand_cont', 'feel_rep_cand_cont',\n",
    "                   'how_many_live_hh_cont', 'better_1y_ago_cont', 'lib_con_scale_cont', \n",
    "                   'soc_spend_favor_cont', 'def_spend_favor_cont', 'private_hi_favor_cont',\n",
    "                   'age_cont', 'schooling_cont']\n",
    "\n",
    "other_predictors = ['int_follow_campg', 'anything_like_dem_cand', 'anything_like_rep_cand', \n",
    "                    'approve_congr', 'things_right_track', 'has_hinsur', 'favor_aca', \n",
    "                    'afraid_dem_cand', 'disgust_dem_cand', 'afraid_rep_cand', 'disgust_rep_cand',\n",
    "                    'incgap_morethan_20y_ago', 'economy_improved', 'unempl_improved', \n",
    "                    'speaksmind_dem_cand', 'speaksmind_rep_cand', 'shoud_hard_buy_gun', \n",
    "                    'favor_affirmaction', 'govt_benefit_all', 'all_ingovt_corrup', \n",
    "                    'election_makegovt_payattn', 'global_warming_happen', 'favor_death_penalty',\n",
    "                    'econ_better_since_2008', 'relig_important', 'married', 'latinx',\n",
    "                    'white', 'black', 'both_parents_bornUS', 'any_grandparent_foreign',\n",
    "                    'rent_home', 'has_unexp_passap', 'should_roughup_protestors', \n",
    "                    'justified_useviolence', 'consider_self_feminist', 'ppl_easily_offended_nowadays',\n",
    "                    'soc_media_learn_pres', 'satisfied_life']\n",
    "## Targets\n",
    "targetvote = 'vote_pres_2016'\n",
    "targetswing_2016 = 'swing_2016'\n",
    "targetswing_2012_2016 = 'swing_2016_2012'\n",
    "dat[targetvote] = dat[targetvote].map({'Johnson': 'Other', 'Stein': 'Other', \n",
    "                                       'Trump': 'Trump', 'Clinton': 'Clinton'})\n",
    "\n",
    "## Level Target\n",
    "levels_target3cand = ['Clinton', 'Other', 'Trump']\n",
    "levels_target2cand = ['Clinton', 'Trump']\n",
    "levels_target_swing = ['Non-Swingers', 'Swingers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37257efd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ae86a26f1df842e12a136e8c06bda8c",
     "grade": false,
     "grade_id": "cell-c8dcb7c7b9ab5d5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 01\n",
    "\n",
    "### Fit a KNN model that predicts vote in the 2016 election based on the feeling thermometer (`feel_rep_cand_cont`) and the age (`age_cont`).\n",
    "\n",
    "Below, I prepared the data for you. Parameters:\n",
    "\n",
    "1. Find the optimal K between 1 and 21, 2 by 2. Check the variable `bigK` that I prepared for you.\n",
    "    + Use K Fold cross-validation to search for the best K.\n",
    "\n",
    "2. Plot the optimal K\n",
    "\n",
    "3. Fit the model with the optimal K and use cross-validation, saving 35 percent of the data for your testing set.\n",
    "\n",
    "4. Print the confusion matrix and the classification report.\n",
    "\n",
    "5. Discuss your findings in terms of the prediction achieved.\n",
    "\n",
    "Hints: you can use the code I did in class, or you can try to use [`GridSearchCV`](https://scikit-learn.org/0.16/modules/generated/sklearn.grid_search.GridSearchCV.html). For grid search, check the following code:\n",
    "\n",
    "```\n",
    "# Assume bigK is our list of parameters, and has been defined.\n",
    "# Then, you can run:\n",
    "knn = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': bigK}\n",
    "model = GridSearchCV(knn, param_grid = parameters, cv = 5)\n",
    "model.fit(X, y)\n",
    "print('Best value of K is ', model.best_params_)\n",
    "```\n",
    "\n",
    "Note that in parameters, you can have many more things. For example, you could change the way distance is computed to check if this affects anything. The parameters for KNN are in [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "\n",
    "One example of search that changes the K but also the weights is:\n",
    "\n",
    "```\n",
    "# Assume bigK is our list of parameters, and has been defined.\n",
    "# Then, you can run:\n",
    "knn = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': bigK, 'weights': ['uniform', 'distance']}\n",
    "model = GridSearchCV(knn, param_grid = parameters, cv = 5)\n",
    "model.fit(X, y)\n",
    "print('Best parameter combination is ', model.best_params_)\n",
    "```\n",
    "\n",
    "Another useful hint: You can [plot your Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator) using the following code:\n",
    "\n",
    "```\n",
    "## Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(estimation_model, X_test, y_test,\n",
    "        display_labels = ['level 1, 'level 2', ..., 'level n'], # How many levels in your target variable :)\n",
    "        cmap = plt.cm.Blues, normalize = 'true') # Making it pretty\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043d5c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f38a2f5087dda12d6777946ae972d61",
     "grade": false,
     "grade_id": "cell-e00ad1a545802416",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep\n",
    "bigK = list(range(1, 22, 2))\n",
    "y = dat[targetvote]\n",
    "X = StandardScaler().set_output(transform = 'pandas').fit_transform(dat[['feel_rep_cand_cont', 'age_cont']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)\n",
    "\n",
    "# Recalling:\n",
    "# Step 1: use the training set + gridsearchCV to find best parameters\n",
    "# Step 2: fit in the testing set and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61a4a8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e847f3adfc72455727e53b3b6ca8381f",
     "grade": true,
     "grade_id": "cell-f5c6c56025002692",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44da62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bcea20f550239ac8f6ca574010d5fb0",
     "grade": false,
     "grade_id": "cell-99df61b10e7e0774",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 02\n",
    "\n",
    "### Fit a KNN model that predicts vote in the 2016 election using a spline on both these variables.\n",
    "\n",
    "Tuning objective:\n",
    "\n",
    "1. Find the optimal number of knots, from 5 to 15.\n",
    "\n",
    "Below, I prepared the data for you.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "To run efficiently, we can build a pipeline. They way you do is the following:\n",
    "\n",
    "```\n",
    "# Classifier: KNN\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Spline\n",
    "splines = SplineTransformer(degree = 3, extrapolation = 'constant')\n",
    "\n",
    "# Parameters for pipeline (can be set using '__' separated parameter names):\n",
    "param_grid = {\n",
    "    'splines__n_knots': nknots,\n",
    "    'splines__knots': ['quantile', 'uniform'],\n",
    "    'splines__extrapolation': ['constant', 'linear'],\n",
    "    'knn__n_neighbors': bigK, \n",
    "    'knn__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Build the pipeline:\n",
    "pipe = Pipeline(steps = [('splines', splines), ('knn', knn)])\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs = -1)\n",
    "search.fit(X, y)\n",
    "\n",
    "# Results\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "```\n",
    "\n",
    "The pipeline can be as complicated as you wish it to be.\n",
    "\n",
    "To more about that, please check this useful code [here](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html).\n",
    "\n",
    "\n",
    "**Question:** Are you doing better using a spline? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13185990",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07669fb75b75c835b458ee16af8cf3b7",
     "grade": false,
     "grade_id": "cell-8807109564343777",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep\n",
    "bigK = list(range(1, 22, 2))\n",
    "nknots = list(range(5, 16))\n",
    "knotstypes = ['quantile', 'uniform']\n",
    "extraptypes = ['constant', 'linear'] # regular x natural spline\n",
    "y = dat[targetvote]\n",
    "X = StandardScaler().set_output(transform = 'pandas').fit_transform(dat[['feel_rep_cand_cont', 'age_cont']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)\n",
    "\n",
    "# Recalling:\n",
    "# Step 1: use the training set + gridsearchCV to find best parameters\n",
    "# Step 2: fit in the testing set and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5435f96",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e82563d23be4f3ef3846a040df49576",
     "grade": true,
     "grade_id": "cell-e8dd3ba8a6ea8ae3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45266d2c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ec8f6cb5ef1ca8c615654c06c46b6b5",
     "grade": false,
     "grade_id": "cell-253cb3127b875bcd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 03\n",
    "\n",
    "### Fit a KNN model that predicts vote in the 2016 election using all variables.\n",
    "\n",
    "- Tuning objective: find optimal K.\n",
    "\n",
    "**Question:** Are you doing better with more variables? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6659c86",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0eea54f877f2b72dedb5fc5762b35c3f",
     "grade": false,
     "grade_id": "cell-c7819eccded9803a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep\n",
    "bigK = list(range(1, 22, 2))\n",
    "y = dat[targetvote]\n",
    "\n",
    "# Continuous predictors: standardize\n",
    "X_cont = StandardScaler().set_output(transform = 'pandas').fit_transform(dat[cont_predictors])\n",
    "\n",
    "# Put together with other predictors\n",
    "X = dat[other_predictors].join(X_cont)\n",
    "\n",
    "# Save some portion for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398c04b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57911030ac53eb169582c89a1dbb0ff3",
     "grade": true,
     "grade_id": "cell-f7e063f043eb7e57",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee022a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b706075332313d22c6449dc3411930f1",
     "grade": false,
     "grade_id": "cell-c6794ad767d0648e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 04\n",
    "\n",
    "### Fit a KNN model that predicts vote on Trump and Clinton in the 2016 election, using all variables\n",
    "\n",
    "I decided to make your life easier (believe me, nothing good ever come after this sentence!). You only need to predict the votes for Clinton and Trump.\n",
    "\n",
    "- Tuning objective: find optimal K.\n",
    "\n",
    "**Question:** How the quality of the prediction compares to the model in Q3? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6956f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a1cdca52dc79b6f768e99f91a87d685",
     "grade": false,
     "grade_id": "cell-fd286369a519293f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep\n",
    "bigK = list(range(1, 22, 2))\n",
    "new_targets = ['Clinton', 'Trump']\n",
    "dat2 = dat.loc[dat[targetvote].isin(new_targets)]\n",
    "y = dat2[targetvote]\n",
    "\n",
    "# 1. Process the continuous variables and make them standardized\n",
    "X_cont = StandardScaler().set_output(transform = 'pandas').fit_transform(dat2[cont_predictors])\n",
    "\n",
    "# 2. Join with all others and compose the full outcome dataset\n",
    "X = dat2[other_predictors].join(X_cont)\n",
    "\n",
    "# Save some portion for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5805ff6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e320b7c4ffc7720a6179c37f8ba65749",
     "grade": true,
     "grade_id": "cell-81351dd96ff64ac4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fea3df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75ffa0f4346a44d553ec8accf816e236",
     "grade": false,
     "grade_id": "cell-742677376a42d56f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 05\n",
    "\n",
    "### Can you do better in the Clinton x Trump classification?\n",
    "\n",
    "Try:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. LDA\n",
    "3. QDA\n",
    "4. Naïve Bayes\n",
    "\n",
    "**Question:** Are we better-off when compared with the K-NN? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6a217",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "259a27db7e72a3c5562e41e133707f4f",
     "grade": false,
     "grade_id": "cell-3d6b263d962994b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep\n",
    "new_targets = ['Clinton', 'Trump']\n",
    "dat2 = dat.loc[dat[targetvote].isin(new_targets)]\n",
    "y = dat2[targetvote]\n",
    "\n",
    "# 1. Process the continuous variables and make them standardized\n",
    "X_cont = StandardScaler().set_output(transform = 'pandas').fit_transform(dat2[cont_predictors])\n",
    "\n",
    "# 2. Join with all others and compose the full outcome dataset\n",
    "X = dat2[other_predictors].join(X_cont)\n",
    "\n",
    "# Save some portion for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c0f22",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "659d291bf6dc8a2e3c10922cbaed189d",
     "grade": true,
     "grade_id": "cell-2837286569667afb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce601cb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5184923c8ea02d91c632dd6737da4c7d",
     "grade": false,
     "grade_id": "cell-264171da48a7541e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 06\n",
    "\n",
    "### Tuned Logistic Regression\n",
    "\n",
    "In the ML world, they usually add two penalties (or [regularizations](https://en.wikipedia.org/wiki/Regularization_(mathematics)#Other_uses_of_regularization_in_statistics_and_machine_learning), if you prefer this word) to our optimization functions: L1 and L2.\n",
    "\n",
    "**L1**: Works like the Lasso in Linear Regressions.\n",
    "\n",
    "**L2**: Works like the Ridge in Linear Regressions.\n",
    "\n",
    "The next one combines both penalties. It is called **elasticnet**.\n",
    "\n",
    "In this question, your job is to fit a Logistic Regression, searching for the best L1 tuning parameter. Note that in the LR world, they call this parameter C. It is the inverse of the alpha that we studied ($\\alpha = \\dfrac{1}{C}$).\n",
    "\n",
    "**Question:** Are we better-off now, when comparing with the un-tuned LR? Explain.\n",
    "\n",
    "Hint: You will need some extra parameters for your Logistic Regression to work within these specifications. Here is a good starting point:\n",
    "\n",
    "```\n",
    "logreg = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 1000000)\n",
    "```\n",
    "\n",
    "In here, we set the penalty, the solver, and the maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf54465",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "446f6dd4cb672e85d96d9996aece897f",
     "grade": false,
     "grade_id": "cell-c787faf632d6d104",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep\n",
    "Cs = np.logspace(-2, 2, 100)\n",
    "new_targets = ['Clinton', 'Trump']\n",
    "dat2 = dat.loc[dat[targetvote].isin(new_targets)]\n",
    "y = dat2[targetvote]\n",
    "\n",
    "# 1. Process the continuous variables and make them standardized\n",
    "X_cont = StandardScaler().set_output(transform = 'pandas').fit_transform(dat2[cont_predictors])\n",
    "\n",
    "# 2. Join with all others and compose the full outcome dataset\n",
    "X = dat2[other_predictors].join(X_cont)\n",
    "\n",
    "# Save some portion for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c023a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6aa2039e5132985b1c263bfa52fd8d91",
     "grade": true,
     "grade_id": "cell-dd861f460a487cba",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc088209",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cb955b9eadfab4de02b3b0495642a53",
     "grade": false,
     "grade_id": "cell-139d5c0be190fd1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 07\n",
    "\n",
    "### Tuned Logistic Regression\n",
    "\n",
    "Plot the coefficients. Do they make sense for you? Hint: use the `sns.barplot`. Some piece of code for you to get started:\n",
    "\n",
    "```\n",
    "# Size of the plot #\n",
    "f, ax = plt.subplots(figsize=(6, 15)) # Good for oversized plots (with lots of variables :)\n",
    "\n",
    "# Nice colors\n",
    "sns.set_color_codes(\"pastel\")\n",
    "\n",
    "# Note that I inverted the x and y axes! #\n",
    "sns.barplot(y = my_testing_or_training_dataset.columns, x = my_regression_mode.coef_[0], color=\"b\")\n",
    "\n",
    "# Add a beautiful line at zero! #\n",
    "ax.axvline(0, color = 'black', ls = 'dotted', lw = 0.5)\n",
    "\n",
    "# Declutter #\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Done #\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc039bac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee6ab2612fe7988338b2980125121388",
     "grade": true,
     "grade_id": "cell-49de24869509945d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebe503",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "816b296e076d8fbae1d51a414c10075b",
     "grade": false,
     "grade_id": "cell-2d3d46e64000b412",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 08\n",
    "\n",
    "### Tuned Logistic Regression\n",
    "\n",
    "In this question, your job is to fit a Logistic Regression, searching for the best **L2** tuning parameter. In this world, L2 is the same as the Ridge in Linear Regression models. Again, they call the alpha parameter C. It is the inverse of the alpha that we studied ($\\alpha = \\dfrac{1}{C}$).\n",
    "\n",
    "**Question:** Are we better-off now, when comparing with the un-tuned LR? How about the **L1** tuned model? Explain.\n",
    "\n",
    "Hint: You will need some extra parameters for your Logistic Regression to work within these specifications. Here is a good starting point:\n",
    "\n",
    "```\n",
    "logreg = LogisticRegression(penalty = 'l2', solver = 'saga', max_iter = 1000000)\n",
    "```\n",
    "\n",
    "In here, we set the penalty, the solver, and the maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4eb03e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b121edda7d6038f09272685b16af8d3f",
     "grade": false,
     "grade_id": "cell-b9f86feb2fa2a961",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep\n",
    "Cs = np.logspace(-2, 2, 100)\n",
    "new_targets = ['Clinton', 'Trump']\n",
    "dat2 = dat.loc[dat[targetvote].isin(new_targets)]\n",
    "y = dat2[targetvote]\n",
    "\n",
    "# 1. Process the continuous variables and make them standardized\n",
    "X_cont = StandardScaler().set_output(transform = 'pandas').fit_transform(dat2[cont_predictors])\n",
    "\n",
    "# 2. Join with all others and compose the full outcome dataset\n",
    "X = dat2[other_predictors].join(X_cont)\n",
    "\n",
    "# Save some portion for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4c1eb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8509e4f81248d070a9bb91340df16423",
     "grade": true,
     "grade_id": "cell-2248ebb57c02440f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357eee18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c38d6f64d6d37549f88ee71aba739f2",
     "grade": false,
     "grade_id": "cell-fb2e7dabb6ccde09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 09\n",
    "\n",
    "### Tuned Logistic Regression\n",
    "\n",
    "Plot the coefficients. Do they make sense for you? Hint: use the **Q7** code hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267bd30",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "273e8d96a466a17d1708e0d2c86c0437",
     "grade": true,
     "grade_id": "cell-90b158bffbc3f170",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f759a3c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ba723ee7ac05f27b0aff75904342214",
     "grade": false,
     "grade_id": "cell-593a5da1e9a50852",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 10\n",
    "\n",
    "### Predicting Swing Voters\n",
    "\n",
    "Now you job just got considerably harder: you need to predict swing voters. You should start with swing between the two waves of the survey and then move to the swing from 2012 elections.\n",
    "\n",
    "#### Between the two waves\n",
    "\n",
    "- The Anes run the survey at two different points: before the election and after the election. They respondents are the same. There are, then, two opportunities to swing: \n",
    "    - First, from one wave to the other. The idea is that between the two waves a voter could change her mind.\n",
    "    - Second, between 2012 and 2016. We will look at the first and then at the second swing and try to predict it.\n",
    "    \n",
    "- Use Logistic Regression in here.\n",
    "\n",
    "Objectives within each model:\n",
    "\n",
    "1. Tuning objectives: Use L2 regularization, seaching for the best parameter.\n",
    "\n",
    "2. Plot and discuss the coefficients.\n",
    "\n",
    "**Question:** Are you confident in your prediction capacity? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5894d7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1cd06fb7ed82a0fd079abe7635e8ec6",
     "grade": false,
     "grade_id": "cell-be65c8eae4442878",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep for model between two waves\n",
    "Cs = np.logspace(-2, 2, 100)\n",
    "level_swingers = ['Non-swingers', 'Swingers']\n",
    "y = dat[targetswing_2016]\n",
    "X_cont = StandardScaler().set_output(transform = 'pandas').fit_transform(dat[cont_predictors])\n",
    "X = dat[other_predictors].join(X_cont)\n",
    "\n",
    "## Save some portion for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)\n",
    "\n",
    "## Solution between waves below ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8fcc8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ced0b23ccfd603f1ba5a0bac74a2348d",
     "grade": true,
     "grade_id": "cell-5a90c1336901232e",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a40492",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18c608e594971a80b0997ffa17c42cb5",
     "grade": false,
     "grade_id": "cell-020a89d272c59857",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Data Prep for model from 2012 to 2016\n",
    "y = dat[targetswing_2012_2016]\n",
    "X_cont = StandardScaler().set_output(transform = 'pandas').fit_transform(dat[cont_predictors])\n",
    "X = dat[other_predictors].join(X_cont)\n",
    "\n",
    "# Save some portion for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)\n",
    "\n",
    "# Solution between 2012 and 2016 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608749c2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95970d68521b83a683ec7bb738b36128",
     "grade": true,
     "grade_id": "cell-44585e0dd9329399",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
